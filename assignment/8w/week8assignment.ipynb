{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from numpy import random\n",
    "from sklearn.svm import SVC\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.ensemble as ske\n",
    "from operator import itemgetter\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 写一个函数，将一个多类别变量转化为多个二元虚拟变量，不用 sklearn 库\n",
    "#### (1) 为什么在聚类中使用虚拟变量？\n",
    "\n",
    "- 如果不把分类变量转为虚拟变量，可能的一个方法，是把分类变量转为连续型的数字，如0，1，2，但这样会产生一些问题。\n",
    "  举个例子，三个分类变量黄绿紫的编码示例：  \n",
    "\n",
    "  类   | 连  |  虚     \n",
    "  黄   |    0        | 1 0 0     \n",
    "  绿     |    1        | 0 1 0     \n",
    "  紫     |    2        | 0 0 1    \n",
    "\n",
    "  如果用连续变量来表示黄绿紫三个变量，很明显，黄紫之间的距离为2，黄绿之间的距离为1。这样，用连续变量来编码分类变量，也就对权重平等的名义变量造成了不必要的距离差异。然而，设置虚拟变量则可避开这一问题。\n",
    "\n",
    "#### (2) 在聚类中使用虚拟变量的好处？\n",
    " \n",
    "- 虚拟变量使得聚类中对名义变量的欧氏距离计算变得可能，也就是说，我们可以把区间变量和名义变量结合到聚类结果中。     \n",
    "  如果不用虚拟变量，会产生怎样的变化？可能会产生用k－means对待区间变量，用k－modes来处理名义特征。    \n",
    "  在聚类中，遇到像性别和种族这样的名义变量，和像年龄收入这样的区间变量，这样的不同会干扰结果解析。\n",
    "\n",
    "- 再说说两分编码法 (0, 1) 的缺点。比如，它会大大增加特征空间的维度，也就会人工地增加样本间的距离。要避免这一问题，可以考虑使用k-prototypes方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 把例子中的分类变量黄绿紫转化为二元虚拟变量，用到 pandas get_dummies() 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>green</th>\n",
       "      <th>purple</th>\n",
       "      <th>yellow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   green  purple  yellow\n",
       "0      0       0       1\n",
       "1      0       1       0\n",
       "2      1       0       0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['yellow', 'purple', 'green']).str.get_dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 如变量不只有黄，绿，紫，而是有黄紫，绿紫，黄，绿，紫这样的分类，该怎么转化为二元虚拟变量？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>green</th>\n",
       "      <th>purple</th>\n",
       "      <th>yellow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   green  purple  yellow\n",
       "0      0       1       1\n",
       "1      1       1       0\n",
       "2      0       0       1\n",
       "3      0       1       0\n",
       "4      1       0       0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(['purple|yellow', 'purple|green', 'yellow', 'purple', 'green']).str.get_dummies()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. 自己写一个函数，实现虚拟变量转换\n",
    "思路：\n",
    "0. 创建变量 list\n",
    "1. 依据变量 list 的特点，创建矩阵并写入 dataframe    \n",
    "🌰 如变量为黄紫，黄绿，黄，紫，绿，行数为5（变量 list 中的所有元素），列数为3（独特的元素）\n",
    "2. 将变量 list 填充到每一列数据中\n",
    "3. 遍历每列，如果数据包涵了列名，返回1，未包含，返回0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purple</th>\n",
       "      <th>green</th>\n",
       "      <th>yellow</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[purple, yellow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[purple, green]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[yellow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[purple]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[green]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   purple  green  yellow          variable\n",
       "0       1      0       1  [purple, yellow]\n",
       "1       1      1       0   [purple, green]\n",
       "2       0      0       1          [yellow]\n",
       "3       1      0       0          [purple]\n",
       "4       0      1       0           [green]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctgs = [['purple', 'yellow',], ['purple', 'green'], ['yellow'], ['purple'], ['green'] ]\n",
    "def dummy_var(lsts):\n",
    "    var = set()\n",
    "    for lst in lsts:\n",
    "        for subctg in lst:\n",
    "            var.add(subctg)\n",
    "    sj = pd.DataFrame(np.random.randn(len(lsts), len(var)), columns = var)\n",
    "    for col in sj.columns:\n",
    "        sj[col] = lsts\n",
    "        sj[col] = sj[col].map(lambda x:1 if col in x else 0)\n",
    "    sj['variable'] = lsts \n",
    "    return sj\n",
    "dummy_var(ctgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 写一个函数，实现交叉验证的功能，不使用sklearn库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### (1) 使用交叉验证的原因    \n",
    "让我们通过以下几幅图来理解这个问题：    \n",
    "<img src=\"http://scikit-learn.org/stable/_images/plot_underfitting_overfitting_001.png\" width=\"70%\" height=\"100%\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 此处我们试图找到 x 和 y 的关系。三个模型各自做了如下工作：\n",
    "- **第一个模型**：此模型使用了线性等式。对于训练用的数据点，这样的模型有很大误差。这是“拟合不足”(“Under fitting”）的一个例子，不足以发掘数据背后的趋势。\n",
    "- **第二个模型**：该模型发现了 x 和 y 的正确关系，误差低/概括程度高。\n",
    "- **第三个模型**：这个模型对于训练数据几乎是零误差。因为，此关系模型把每个数据点的偏差（包括噪声）都纳入了考虑范围，也就是说，这个模型太过敏感，甚至会捕捉到只在当前数据训练集出现的一些随机模式。这是“过度拟合”（“Over fitting”）的一个例子。\n",
    "- **交叉验证**：在数据科学竞赛中，一个常见的做法是对多个模型进行迭代，从中选择表现更好的。然而，最终的分数是否会有改善依然未知，因为我们不知道这个模型是更好的发掘潜在关系了，还是过度拟合了。为了解答这个难题，我们应该使用交叉验证（cross validation）技术。它能帮我们得到更有概括性的关系模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 什么是交叉验证？\n",
    "- 交叉验证意味着需要保留一个样本数据集，不用来训练模型。在最终完成模型前，用这个数据集验证模型。\n",
    "- 交叉验证步骤：   \n",
    "  a.保留一个样本数据集   \n",
    "  b.用剩余部分训练模型   \n",
    "  c.用保留的数据集验证模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 交叉验证的常用方法是什么？\n",
    "\n",
    "####  **Hold Out 法**\n",
    "   - 保留一定比例的数据集用作验证，剩下的训练模型，再用验证集测试模型表现\n",
    "   - 缺陷\n",
    "       - 由于只使用了一定比例的数据训练模型，原数据中一些重要的信息可能被忽略     \n",
    "       - 从严格意义上来说，Hold-Out 检验不算是交叉检验，因为该方法没有达到交叉检验的思想，而且最后验证准确性的高低和原始数组的分类有很大的关系，所以该方法得到的结果在某些场景中并不具备特别大的说服力\n",
    "\n",
    "#### 一个简单的函数实现\n",
    "上周 titanic 数据的一个练习中，实现过非 sklearn库的 hold out法交叉验证，这里总结为一个函数：   \n",
    "先对数据进行排列，接着把数据为两个子集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hold_out(X,y,size):\n",
    "# 0 < size < 1\n",
    "    order = np.random.permutation(len(x))\n",
    "    X = X[order]\n",
    "    y = y[order].astype(np.float)\n",
    "    X_train = X[:int(size * len(x))]\n",
    "    y_train = y[:int(size * len(x))]\n",
    "    X_test = X[int(size * len(x)):]\n",
    "    y_test = y[int(size * len(x)):]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用 sklearn 库中的其他分类方法，来预测 titanic 的生存情况\n",
    "上周已用过 支持向量机 SVM 和 随机森林两种方法，代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 支持向量机算法预测评分 : 0.775280898876\n"
     ]
    }
   ],
   "source": [
    "# 数据导入和清理\n",
    "train = pd.read_csv(\"train.csv\") \n",
    "train = train.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis = 1)\n",
    "train['Age'] = train['Age'].fillna(train['Age'].median())\n",
    "train= train.dropna(how = 'any')\n",
    "\n",
    "# 设立公式，建立数据框\n",
    "formula =  'Survived ~ C(Pclass) + C(Sex) + Age + SibSp' \n",
    "y, x = dmatrices(formula, data=train, return_type='matrix')\n",
    "\n",
    "# 将公式数据框中的数据放入 numpy array\n",
    "# 挑选感兴趣的变量来分析:通过刚才的回归分析，已知性别和船舱对是否生还的影响较大，这里选择代表性别 3 和船舱 2 的两列来分析\n",
    "X = np.asarray(x)\n",
    "# y 值是这样的数据，array([[ 0.], [ 1.], [ 1.], [ 1.] ...]), 用 np.ravel() 转换为 array([ 0.,  1.,  1.,  1.,  0.,  0., ...])\n",
    "y = np.asarray(y).ravel()\n",
    "X = X[:,[2, 3]]  \n",
    "\n",
    "# 随机排列 x, y值 \n",
    "np.random.seed(123)\n",
    "order = np.random.permutation(len(x))\n",
    "X = X[order]\n",
    "y = y[order].astype(np.float)\n",
    "\n",
    "# 做一个交叉验证，前80%的数据用来训练，后20%的数据用来检验\n",
    "X_train = X[:int(.8 * len(x))]\n",
    "y_train = y[:int(.8 * len(x))]\n",
    "X_test = X[int(.8 * len(x)):]\n",
    "y_test = y[int(.8 * len(x)):]\n",
    "\n",
    "# 1. 支持向量机\n",
    "# gamma 参数: 参数 gamma 表示超平面的线性平滑度，当使用线性核函数构建 SVM 时，模型中不存在 gamma 参数。\n",
    "# 我们知道: gamma 越小，超平面越接近于直线，但是如果 gamma 选取过大，超平面将会变得非常弯曲，这会导致过度拟合问题。\n",
    "poly = SVC(kernel='poly', gamma=2.5)\n",
    "poly.fit(X_train, y_train)\n",
    "print u'1. 支持向量机算法预测评分 :',poly.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. 随机森林预测评分 : 0.902137232846\n"
     ]
    }
   ],
   "source": [
    "# 创建随机森林模型\n",
    "Forest_formula = 'Survived ~ C(Pclass) + C(Sex) + Age + SibSp' \n",
    "y, x = dmatrices(Forest_formula, data=train, return_type='dataframe')\n",
    "# y 转 1 维\n",
    "y = np.asarray(y).ravel()\n",
    "# 把数据拟合进拟合模型\n",
    "results = ske.RandomForestClassifier(n_estimators=100).fit(x, y)\n",
    "\n",
    "# 显示结果，精确度很理想，为0.9\n",
    "print u'2. 随机森林预测评分 :',results.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 研究 kaggle 中的 Digit Recognizer 数据，尝试用一些特征工程来提取数字的特征，并放入分类器中观察预测准确率，相对直接使用原始变量是否有提升\n",
    "#### (1) 数据导入与初探\n",
    "digit recognition 的数据从 sklearn 数据集中导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 样本大小和特征数量: (1797, 64), 图片个数: (1797,)\n",
      "   包含的数字: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "2. 随机查看三张图片:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAB+CAYAAAD4FtBqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w/RQgKSHmF4EQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1236a5310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 导入数据集\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# 查看有多少张图片\n",
    "print(u\"1. 样本大小和特征数量: %r, 图片个数: %r\" % (X.shape, y.shape))\n",
    "print(u\"   包含的数字: %r\" % list(np.unique(y)))\n",
    "\n",
    "# 随机查看三张图片\n",
    "print u'2. 随机查看三张图片:'\n",
    "fig, (pic1, pic2, pic3) = plt.subplots(1, 3)\n",
    "pic1.imshow(digits.images[random.randint(n_samples)], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "pic2.imshow(digits.images[random.randint(n_samples)], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "pic3.imshow(digits.images[random.randint(n_samples)], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) randomizedPCA 提取特征\n",
    "原来有 64 个特征值，对特征值进行降维，提取主要特征1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 20)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=20)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 交叉验证\n",
    "前80%的数据做训练用，后20％的数据做验证用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,random_state=12345)\n",
    "Xpca_train, Xpca_test = train_test_split(X_pca,train_size=0.8,random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 分类及特征工程前后对比\n",
    "SVM 支持向量机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取之间的预测评分 0.9916666666666667, 特征提取之后的预测评分 0.99444444444444446, 预测准确率有所提升\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='rbf', C=100, gamma=0.001)\n",
    "svc_fit = svc.fit(X_train, y_train)\n",
    "SVM_score = svc_fit.score(X_test, y_test) \n",
    "\n",
    "svc1 = SVC(kernel='rbf', C=100, gamma=0.001).fit(Xpca_train, y_train)\n",
    "svc1_fit = svc1.fit(Xpca_train, y_train)\n",
    "SVM_score_pca = svc1_fit.score(Xpca_test, y_test) \n",
    "\n",
    "print u'特征提取之间的预测评分 %r, 特征提取之后的预测评分 %r, 预测准确率有所提升' %(SVM_score, SVM_score_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) 查看预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从验证数据集里抽取一张图片："
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAACMCAYAAACnK+FEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC8QJ4gJxgrhAnCAuECeIC8QJ4gJxgvwfWcLGfSakRQcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1237c2210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 预测结果:  [9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Q16/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print u'从验证数据集里抽取一张图片：',\n",
    "plt.figure(1, figsize=(2, 2))\n",
    "plt.imshow(digits.images[-2], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "print u'预测结果: ', svc.predict(digits.data[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 用 numpy 库实现 K-nearest neighbors 分类\n",
    "#### (1) 什么是 KNN 算法？\n",
    "- K Nearest Neighbor算法又叫KNN算法，是机器学习里面一个比较经典且容易理解的算法。其中的 K 表示最接近自己的 K 个数据样本。\n",
    "- KNN 算法和 K-Means 算法不同的是，K-Means 算法用来聚类，用来判断哪些东西是一个比较相近的类型，而 KNN 算法是用来做归类的，也就是说，有一个样本空间里的样本分成很几个类型，然后，给定一个待分类的数据，通过计算接近自己最近的 K 个样本来判断这个待分类数据属于哪个分类。\n",
    "- 可以简单将 KNN 算法理解为，数据点 A 属于哪一类，由离它最近的K个点来投票，投票结果决定 A 该被归为哪一类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一个例子\n",
    "<img src=\"http://coolshell.cn//wp-content/uploads/2012/08/220px-KnnClassification.svg_.png\" width=\"27%\" height=\"100%\" style=\"float:left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图中的有两类数据，正方形和三角形；圆形为待分类的数据。\n",
    "- K=3，离圆形点最近的有2个三角形和1个正方形，则由这3个点投票，于是圆形的待分类点属于三角形\n",
    "- K=5，那么离圆形点最近的有2个三角形和3个正方形，由5个点投票，于是圆形的待分类点属于正方形。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 选取了 iris 数据来进行 KNN 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying test instance number 0: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 1: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 2: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 3: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 4: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 5: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 6: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 7: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 8: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 9: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 10: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 11: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 12: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 13: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 14: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 15: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 16: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 17: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 18: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 19: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 20: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 21: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 22: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 23: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 24: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 25: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 26: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 27: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 28: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 29: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 30: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 31: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 32: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 33: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 34: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 35: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 36: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 37: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 38: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 39: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 40: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 41: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 42: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 43: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 44: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 45: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 46: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 47: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 48: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 49: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 50: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 51: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 52: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 53: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 54: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 55: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 56: Predicted label=1, Actual label=2\n",
      "Classifying test instance number 57: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 58: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 59: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 60: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 61: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 62: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 63: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 64: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 65: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 66: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 67: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 68: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 69: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 70: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 71: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 72: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 73: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 74: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 75: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 76: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 77: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 78: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 79: Predicted label=1, Actual label=1\n",
      "Classifying test instance number 80: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 81: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 82: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 83: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 84: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 85: Predicted label=2, Actual label=1\n",
      "Classifying test instance number 86: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 87: Predicted label=2, Actual label=2\n",
      "Classifying test instance number 88: Predicted label=0, Actual label=0\n",
      "Classifying test instance number 89: Predicted label=0, Actual label=0\n",
      "\n",
      "The overall accuracy of the model is: 0.977777777778\n",
      "\n",
      "A detailed classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     setosa       1.00      1.00      1.00        30\n",
      " versicolor       0.96      0.96      0.96        28\n",
      "  virginica       0.97      0.97      0.97        32\n",
      "\n",
      "avg / total       0.98      0.98      0.98        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 导入数据，经典花分类数据集\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.6, random_state=1)\n",
    " \n",
    "# 交叉验证\n",
    "train = np.array(zip(X_train,y_train))\n",
    "test = np.array(zip(X_test, y_test))\n",
    " \n",
    "# 1) 已知两个数据点，计算欧氏距离\n",
    "def get_distance(point1, point2):\n",
    "    points = zip(point1, point2)\n",
    "    diffs_squared_distance = [pow(a - b, 2) for (a, b) in points]\n",
    "    return math.sqrt(sum(diffs_squared_distance))\n",
    "\n",
    "def get_neighbours(training_set, test_instance, k):\n",
    "    distances = [_get_tuple_distance(training_instance, test_instance) for training_instance in training_set]\n",
    " \n",
    "    # 1 为训练值与检验值之间的距离 \n",
    "    sorted_distances = sorted(distances, key=itemgetter(1))\n",
    " \n",
    "    # 只抽取训练值\n",
    "    sorted_training_instances = [tuple[0] for tuple in sorted_distances]\n",
    " \n",
    "    # 选择 k 个元素\n",
    "    return sorted_training_instances[:k]\n",
    " \n",
    "def _get_tuple_distance(training_instance, test_instance):\n",
    "    return (training_instance, get_distance(test_instance, training_instance[0]))\n",
    " \n",
    "def get_majority_vote(neighbours):\n",
    "    # index 1 is the class\n",
    "    classes = [neighbour[1] for neighbour in neighbours]\n",
    "    count = Counter(classes)\n",
    "    return count.most_common()[0][0]\n",
    "\n",
    "def main():\n",
    " \n",
    "    predictions = []\n",
    " \n",
    "    # 先随意将k设置为5，即初始尝试的分类值为5\n",
    "    k = 5\n",
    " \n",
    "    for x in range(len(X_test)):\n",
    " \n",
    "            print 'Classifying test instance number ' + str(x) + \":\",\n",
    "            neighbours = get_neighbours(training_set=train, test_instance=test[x][0], k=5)\n",
    "            majority_vote = get_majority_vote(neighbours)\n",
    "            predictions.append(majority_vote)\n",
    "            print 'Predicted label=' + str(majority_vote) + ', Actual label=' + str(test[x][1])\n",
    " \n",
    "    # 总结\n",
    "    print '\\nThe overall accuracy of the model is: ' + str(accuracy_score(y_test, predictions)) + \"\\n\"\n",
    "    report = classification_report(y_test, predictions, target_names = iris.target_names)\n",
    "    print 'A detailed classification report: \\n\\n' + report\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 笔记\n",
    "\n",
    "#### (1) 交叉验证其他方法\n",
    "\n",
    "#### **留一 ( LOOCV )**\n",
    "\n",
    "   - 步骤1: 只保留一个数据点用作验证，用剩余的数据集训练模型；步骤2: 对每个数据点重复步骤1。\n",
    "   - 优点：使用了所有数据点，因此误差较小\n",
    "   - 缺点：- 验证过程重复了 n 次（ n 为数据点个数），导致执行时间很长；由于只使用了一个数据点验证，该方法导致模型有效性的差异更大。得到的估计结果深受此点的影响。如果这是个离群点，会引起较大偏差。\n",
    "   \n",
    "####   **K 层交叉验证法 (K- fold cross validation)**\n",
    "\n",
    "- 总结前两种验证方法：\n",
    "    - a.训练模型时，应该使用一定比例的数据集来训练模型，否则可能会创建出偏误较大的模型\n",
    "    - b.验证用的数据点，其比例应该恰到好处。如果太少，会导致验证模型有效性时，得到的结果波动较大。\n",
    "    - c.训练和验证过程应该重复多次。训练集和验证集不能一成不变。这样有助于验证模型有效性。\n",
    " \n",
    " \n",
    "- K 层交叉验证 -- 兼顾上述三个方面的交叉验证法\n",
    "     - 定义：把整个数据集随机分成 K“层”，用其中 K-1 层训练模型，然后用第K层验证；记录从每个预测结果获得的误差；重复这个过程，直到每“层”数据都作过验证集；记录下的 k 个误差的平均值，被称为交叉验证误差（cross-validation error）。可以被用做衡量模型表现的标准。\n",
    "\n",
    "\n",
    "- “如何确定合适的k值？”    \n",
    "     - K 值越小，偏误越大；K 值太大，所得结果会变化多端\n",
    "     - K 值小，会变得像“验证集法”；K 值大，则会变得像“留一法”（LOOCV）。通常建议的分层值是 k=10 \n",
    "   \n",
    "   \n",
    "- 如何衡量模型的偏误/变化程度？     \n",
    "     - K 层交叉检验之后，我们得到 K 个不同的模型误差估算值（e1, e2 …..ek）。理想的情况是，这些误差值相加得 0 。要计算模型的偏误，我们把所有这些误差值相加。平均值越低，模型越优秀。\n",
    "     - 模型表现变化程度的计算与之类似。取所有误差值的标准差，标准差越小说明模型随训练数据的变化越小。\n",
    "     - 我们应该试图在偏误和变化程度间找到一种平衡。降低变化程度、控制偏误可以达到这个目的。这样会得到更好的预测模型。进行这个取舍，通常会得出复杂程度较低的预测模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
